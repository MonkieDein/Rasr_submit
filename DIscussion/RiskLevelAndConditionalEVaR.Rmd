---
title: "Discretize"
author: "EVaR"
date: "2022-11-16"
output: 
  pdf_document:
    extra_dependencies: [amsmath,bbm]
---

```{r,include=FALSE}
# setwd("~/Desktop/GITHUB/Rasr_submit")
setwd("C:/GITHUB/Rasr_submit")
source("Code/RASR_code.R")
```

Let denote ERM and EVaR for reward random variable $X$:
\[
\text{ERM}_\beta[X] = -\frac{1}{\beta}\log(\mathbb{E}[e^{-\beta X}])
\]
\[
\text{EVaR}_\alpha[X] = \sup_{\beta > 0}\{ \text{ERM}_\beta[X] + \frac{\log(1-\alpha)}{\beta} \}
\]
The limiting case of the risk measure is stated as
$\lim_{\beta \mapsto 0} \text{ERM}_\beta[X] = \lim_{\alpha \mapsto 0} \text{EVaR}_\alpha[X] = \mathbb{E}[X]$
and 
$\lim_{\beta \mapsto \infty} \text{ERM}_\beta[X] = \lim_{\alpha \mapsto 1} \text{EVaR}_\alpha[X] = \inf[X]$.

The dual notation for EVaR can be written as 
\[
\text{EVaR}_\alpha[X] = \inf_{Z > 0}\{\mathbb{E}[XZ]:\mathbb{E}[Z] = 1 ,\mathbb{E}[Z \log(Z)]\leq -\log(1-\alpha) \}
\]
In this document, we provide an example to dis-prove Theorem 1 and equation (6) from ("EVaR Optimization for Risk-Sensitive
Reinforcement Learning" by Xinyi Ni)
\[
\text{EVaR}_\alpha[X] \neq \inf \{~ \mathbb{E}[\xi_\tau \cdot \text{EVaR}_{1- 
(1-\alpha) \xi_T}[X | \mathcal{F}_\tau] ] ~ : \mathbb{E}[\xi_\tau] = 1 ,\mathbb{E}[\xi_\tau \log( \xi_\tau )] \leq -\log(1-\alpha)\} 
\]

# Example

Define problem
```{r}
X1 = data.frame(x = c(1,2,3,4),p = c(0.1,0.2,0.5,0.2))
X2 = data.frame(x = c(50,60,70,80),p = c(0.5,0.3,0.1,0.1))
condp = c(0.1,0.9)
Xjoint = data.frame(x = c(X1$x,X2$x),p = c(X1$p*condp[1],X2$p*condp[2]))
```


# Define parameter

To define a parameter we know is true, we set an arbitrary $\beta^\star = 0.5$ for ERM. Furthermore, we define $Z^\star = \frac{e^{-\beta^\star X}}{\mathbb{E}[e^{-\beta^\star X}]}$ and compute the $\alpha = 1 - e^{-\mathbb{E}[Z \log(Z)]}$ for EVaR.
```{r}
beta = 0.0763386691745441
logZ = -beta*(Xjoint$x - ERM(Xjoint$x,alpha=beta,prob=Xjoint$p))
alpha = 1 - exp(-E(X=exp(logZ)*logZ,prob=Xjoint$p))
alpha
```

Now we can upper-bound EVaR with $\text{EVaR}_\alpha[X] = \inf_{Z > 0}\{\mathbb{E}[XZ]:\mathbb{E}[Z] = 1 ,\mathbb{E}[Z \log(Z)]\leq -\log(1-\alpha) \} \leq \min_{Z \in \{Z ^\star\}}\{\mathbb{E}[XZ]:\mathbb{E}[Z] = 1 ,\mathbb{E}[Z \log(Z)]\leq -\log(1-\alpha) \} = \mathbb{E}[X Z ^\star]$.
```{r}
Z = exp(logZ)
ub = sum(Z*Xjoint$x*Xjoint$p)
cat("EVaR",alpha,"of X can be upper bound by", ub)
```

Furthermore we can also lower-bound EVaR with $\text{EVaR}_\alpha[X] = \sup_{\beta > 0}\{ \text{ERM}_\beta[X] + \frac{\log(1-\alpha)}{\beta} \} \geq \max_{\beta \in \{\beta^\star\}}\{ \text{ERM}_\beta[X] + \frac{\log(1-\alpha)}{\beta} \} = \text{ERM}_{\beta^\star}[X] + \frac{\log(1-\alpha)}{\beta^\star}$.
```{r}
lb = ERM(Xjoint$x,alpha = beta, prob = Xjoint$p) + log(1-alpha)/beta
cat("EVaR",alpha,"of X can be lower bound by", lb)
```

Note that $\mathbb{E}[X Z ^\star] \geq \text{EVaR}_\alpha[X] \geq \text{ERM}_{\beta^\star}[X] + \frac{\log(1-\alpha)}{\beta^\star}$ for $\beta^\star$ and $Z^\star$ define above. Furthermore, in our example $\mathbb{E}[X Z ^\star] = \text{ERM}_{\beta^\star}[X] + \frac{\log(1-\alpha)}{\beta^\star}$ by sandwich theorem $\mathbb{E}[X Z ^\star] = \text{EVaR}_\alpha[X] = \text{ERM}_{\beta^\star}[X] + \frac{\log(1-\alpha)}{\beta^\star}$.

```{r}
cat("EVaR",alpha,"of X is within [", lb,",", ub ,"]")
```

# Disprove Equation
Above we have computed the EVaR for our example $\text{EVaR}_\alpha[X] = 1.96105138948307$. Now we compute
\[
\inf \{~ \mathbb{E}[\xi_\tau \cdot \text{EVaR}_{1-(1-\alpha) \xi_T}[X | \mathcal{F}_\tau] ] ~ : \mathbb{E}[\xi_\tau] = 1 ,\mathbb{E}[\xi_\tau \log( \xi_\tau )] \leq -\log(1-\alpha)\} 
\]

We can denote our problem with $|\mathcal{F}_\tau| = 2$ as $\mathbb{E}[\xi_\tau]  = p_1\xi_1 + p_2 \xi_2= 1$, and compute $\xi_2 = \frac{1-p_1 \xi_1}{p_2}$. However, only $\xi_\tau$ pair that satisfy $\mathbb{E}[\xi_\tau \log( \xi_\tau )] = p_1\xi_1\log(\xi_1) + p_2 \xi_2\log(\xi_2)\leq -\log(1-\alpha)$ would be plausible value.
```{r}
Xi1 = ((1:999)*0.001)/(1-alpha)
A1 = 1-(1-alpha)*Xi1
# A1 = 0.6482006009142005087398
# Xi1 = (1-A1)/(1-alpha)
Xi2 = (1-condp[1]*Xi1)/condp[2]
A2 = 1-(1-alpha)*Xi2
filter = which(((condp[1]*Xi1*log(Xi1) + condp[2]*Xi2*log(Xi2) ) <= -log(1-alpha))&((1-alpha)*Xi1 <= 1)&((1-alpha)*Xi2 <= 1))
```

Find beta for A2
```{r}
B = 10000*0.99^(1:1000)
ERMb2 = sapply(B,function(b) ERM(X2$x,alpha=b,prob=X2$p))
EVaR2 = max(ERMb2 + log(1-A2)/B)
```

Find beta for A1
```{r}
B = 10000*0.99^(1:1000)
ERMb1 = sapply(B,function(b) ERM(X1$x,alpha=b,prob=X1$p))
EVaR1 = max(ERMb1 + log(1-A1)/B)
which.EVaR1 = which.max(ERMb1 + log(1-A1)/B)
which.EVaR1
b1 = B[which.EVaR1]
logZ1 = -b1*(X1$x - ERM(X1$x,alpha=b1,prob=X1$p))
alp1 = 1-exp(-E(exp(logZ1)*logZ1,X1$p))
EVaR1= E(exp(logZ1)*X1$x,prob=X1$p)
```

Find EVaR
```{r}
E(X = c(Xi1*EVaR1,Xi2*EVaR2),prob=condp)
```


```{r}
# The wrong solution can be computed as
cat("wrong method",E(X = c(zt1 * E(Z1*X1$x,prob=X1$p),zt2 * E(Z2*X2$x,prob=X2$p)),prob = condp))
```

